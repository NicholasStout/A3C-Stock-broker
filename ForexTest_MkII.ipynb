{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import threading\n",
    "import multiprocessing\n",
    "import random\n",
    "from time import sleep\n",
    "from time import time\n",
    "import scipy.signal\n",
    "from env import stock\n",
    "import collections as col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Q_network:\n",
    "    def __init__(self,in_size,out_size,opt,layers=4,width=8, train=True):\n",
    "        self.inputs = tf.placeholder(shape=[None, in_size], dtype=tf.float32)\n",
    "        if layers > 0:\n",
    "            self.a = [tf.layers.dense(self.inputs, width, activation=tf.nn.relu)]\n",
    "            self.v = [tf.layers.dense(self.inputs, width, activation=tf.nn.relu)]\n",
    "            for i in range(layers-1):\n",
    "                self.a.append(tf.layers.dense(self.a[-1], width, activation=tf.nn.relu))\n",
    "                self.v.append(tf.layers.dense(self.v[-1], width, activation=tf.nn.relu))\n",
    "                \n",
    "                self.a.append(tf.layers.dense(self.a[-1], out_size, activation=None))\n",
    "                self.v.append(tf.layers.dense(self.v[-1], out_size, activation=None))\n",
    "\n",
    "        else:\n",
    "            self.a = [tf.layers.dense(self.inputs, out_size, activation=None)]\n",
    "            self.v = [tf.layers.dense(self.inputs, 1, activation=None)]\n",
    "        \n",
    "        self.guessQ = self.v[-1] + tf.subtract(self.a[-1], tf.reduce_mean(self.a[-1],axis=1,keep_dims=True))\n",
    "        self.choice = tf.argmax(self.guessQ, 1)\n",
    "        \n",
    "        if train:\n",
    "            self.o = tf.train.AdamOptimizer(learning_rate=opt)\n",
    "            self.true_Q = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "            self.action = tf.placeholder(shape=[None], dtype=tf.uint8)\n",
    "            self.hot = tf.one_hot(self.action, 3)\n",
    "            \n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.guessQ, self.hot), axis=1)\n",
    "            \n",
    "            #local_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "            self.err = tf.square(self.true_Q - self.Q)\n",
    "            self.loss = tf.reduce_mean(self.err)\n",
    "            self.grads = self.o.compute_gradients(self.loss)\n",
    "            self.train_op = self.o.apply_gradients(self.grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class memories:\n",
    "    def __init__(self, buffer_size=100000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    def add_memory(self, memory):\n",
    "        self.buffer.extend(memory)\n",
    "    def retrieve_memories(self, num):\n",
    "        ret = np.array(random.sample(self.buffer,num))\n",
    "        #print(ret.shape)\n",
    "        #print(ret)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updateTargetGraph(tfVars,tau):\n",
    "    total_vars = len(tfVars)\n",
    "    op_holder = []\n",
    "    for idx,var in enumerate(tfVars[0:total_vars//2]):\n",
    "        op_holder.append(tfVars[idx+total_vars//2].assign((var.value()*tau) + ((1-tau)*tfVars[idx+total_vars//2].value())))\n",
    "    return op_holder\n",
    "\n",
    "def updateTarget(op_holder,sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock = stock()\n",
    "o = 1e-3\n",
    "Q_main = Q_network(14, 3, o)\n",
    "Q_target = Q_network(14, 3, o)\n",
    "\n",
    "e = 1\n",
    "stop = .001\n",
    "decay = (e - stop)/10000\n",
    "n_eps = 50000\n",
    "training_steps = 8\n",
    "step_count = 0\n",
    "group_count = 0\n",
    "update_steps = 4\n",
    "batch_size = 32\n",
    "discount = .99\n",
    "path = './graph/saved'\n",
    "membuf = memories()\n",
    "saver = tf.train.Saver()\n",
    "restore = False\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "target_ops = updateTargetGraph(trainables,.001)\n",
    "\n",
    "\n",
    "rewards = []\n",
    "wl_rate = []\n",
    "err = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if restore:\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    for i in range(n_eps):\n",
    "        stock.get_new_day()\n",
    "        stock.get_first_ohlc()\n",
    "        inp = stock.next_data()\n",
    "        epbuf = memories()\n",
    "        total_reward = 0\n",
    "        count_moves = 0\n",
    "        count_wins = 0\n",
    "        while len(inp) is not 0:\n",
    "            if np.random.rand(1) < e or step_count < training_steps:\n",
    "                a = np.random.randint(0,2)\n",
    "            else:\n",
    "                a = sess.run(Q_main.choice, feed_dict={Q_main.inputs:[inp]}) #fix\n",
    "            r = stock.make_choice(a)\n",
    "            if (r is not 0):\n",
    "                count_moves += 1\n",
    "                if (r > 0):\n",
    "                    count_wins += 1\n",
    "            step_count += 1\n",
    "            inp2 = stock.next_data()\n",
    "            if (len(inp2) is not 0):\n",
    "                epbuf.add_memory(np.reshape(np.array([inp,a,r,inp2]),[1,4]))\n",
    "            total_reward += r\n",
    "            if group_count > training_steps:\n",
    "                if e > stop:\n",
    "                    e = e-decay\n",
    "                if (step_count % update_steps) == 0:\n",
    "                    training_batch = membuf.retrieve_memories(batch_size)\n",
    "                    #print(training_batch[:,3])\n",
    "                    Q_val_1 = sess.run(Q_main.choice, feed_dict={Q_main.inputs:np.vstack(training_batch[:,3])})\n",
    "                    Q_val_2 = sess.run(Q_target.guessQ, feed_dict={Q_target.inputs:np.vstack(training_batch[:,3])})\n",
    "                    \n",
    "                    D_Q = Q_val_2[range(batch_size), Q_val_1]\n",
    "                    target_Q = training_batch[:,2]+(discount*D_Q)\n",
    "                    #print(np.vstack(training_batch[:,1]))\n",
    "                    fd = {Q_main.inputs:np.vstack(training_batch[:,0]),\n",
    "                          Q_main.true_Q:target_Q,\n",
    "                          Q_main.action:np.reshape(np.vstack(training_batch[:,1]),[batch_size])}\n",
    "                    train = sess.run([Q_main.grads,Q_main.train_op,Q_main.loss], \n",
    "                                     feed_dict=fd)\n",
    "                    err.append(train[2])\n",
    "                    updateTarget(target_ops,sess)\n",
    "            inp = inp2\n",
    "        group_count += 1\n",
    "        membuf.add_memory(epbuf.buffer)\n",
    "        rewards.append(total_reward)\n",
    "        if (count_moves is 0):\n",
    "            wl_rate.append(0)\n",
    "        else:\n",
    "            wl_rate.append(count_wins/count_moves)\n",
    "        if(i % 8 == 0):\n",
    "            saver.save(sess, path)\n",
    "            print(np.mean(rewards[-8:]))\n",
    "            print(np.mean(wl_rate[-8:]))\n",
    "            print(np.mean(err[-8:]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a Q-network that is made to try to learn to buy and sell stocks with reinforcement learning. It ultimately failed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as ns\n",
    "from env import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ohlc_rnn(object):\n",
    "    def __init__(self, optimizer, inputs=8,depth=2, hidden=128, keep_prob=0.5):\n",
    "        self.inputs = tf.placeholder(dtype=tf.float32, shape=[None, inputs])\n",
    "        self.LSTM = []\n",
    "        self.state = ()\n",
    "        for i in range (0, depth):\n",
    "            cell = tf.contrib.rnn.BasicLSTMCell(hidden, state_is_tuple=True, forget_bias=keep_prob, activation=tf.nn.relu)\n",
    "            #cell = tf.nn.rnn_cell.DropoutWrapper(cell_0, output_keep_prob=keep_prob)\n",
    "            self.LSTM.append(cell)\n",
    "            self.state = self.state + (tf.placeholder(tf.float32, [1, cell.state_size.c]), \n",
    "                                       tf.placeholder(tf.float32, [1, cell.state_size.h]))\n",
    "        m_cell = tf.contrib.rnn.MultiRNNCell(self.LSTM, state_is_tuple=True)\n",
    "        \n",
    "        self.init_state = self.state = m_cell.zero_state(1, tf.float32)\n",
    "        self.rnn_in = tf.expand_dims(self.inputs, [0])\n",
    "        r_output, self.state_out = tf.nn.dynamic_rnn(cell=m_cell, inputs=self.rnn_in, initial_state=self.state, time_major=False)\n",
    "        d_output = tf.layers.dense(r_output, inputs)\n",
    "        self.output = tf.reshape(d_output, shape=[-1, inputs])\n",
    "        \n",
    "        loss = 0\n",
    "        self.next_ohlc = tf.placeholder(dtype=tf.float32, shape=[None, inputs])\n",
    "        loss = tf.losses.mean_squared_error(labels=self.next_ohlc, predictions=self.output)\n",
    "        self.cost = tf.reduce_mean(loss)*100\n",
    "        \n",
    "        self.grads = optimizer.compute_gradients(self.cost)\n",
    "        self._train_op = optimizer.apply_gradients(self.grads)\n",
    "        \n",
    "    def run_rnn(self, inputs, sess, state):\n",
    "        output, state_ = sess.run([self.output, self.state_out], feed_dict={self.inputs:inputs, self.state:state})\n",
    "        return output, state_\n",
    "            \n",
    "    def train_rnn(self, day, sess):\n",
    "        ohlc, n_ohlc = day.get_next_ohlc()\n",
    "        self.predict, state, ret, train = sess.run([self.output, self.state_out, self.cost, self._train_op],\n",
    "                            feed_dict={self.inputs:ohlc, self.next_ohlc:n_ohlc})\n",
    "        #print(st)\n",
    "        return ret\n",
    "    \n",
    "    def get_init_state(self, sess):\n",
    "        return sess.run([self.init_state], feed_dict={self.state:self.init_state})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forex = forex_env()\n",
    "optimizer = tf.train.AdamOptimizer(1e-9)\n",
    "rnn = ohlc_rnn(optimizer=optimizer)\n",
    "save = tf.train.Saver()\n",
    "train = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "with tf.Session() as sess:\n",
    "    kevin = tf.summary.FileWriter(\"./graph/graph\", sess.graph)\n",
    "    state = sess.run([rnn.init_state])\n",
    "    if (train):\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        while forex.check_if_done():\n",
    "            day = forex.get_d_forex()\n",
    "            loss = rnn.train_rnn(day, sess)\n",
    "            if (i % 60) is 0:\n",
    "                print(loss)\n",
    "                save.save(sess, \"graph/model.ckpt\")\n",
    "            i=i+1\n",
    "        forex.reset()\n",
    "        while forex.check_if_done():\n",
    "            day = forex.get_d_forex()\n",
    "            loss = rnn.train_rnn(day, sess)\n",
    "            if (i % 20) is 0:\n",
    "                print(loss)\n",
    "                save.save(sess, \"graph/model.ckpt\")\n",
    "            i=i+1\n",
    "    else:\n",
    "        save.restore(sess, \"graph/model.ckpt\")\n",
    "    #forex.reset()\n",
    "    day = forex.get_d_forex()\n",
    "    inp, n = day.get_next_ohlc()\n",
    "    print(rnn.run_rnn(inp, sess, state, n))\n",
    "    print(n)\n",
    "    kevin.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This was a linear regressor using LSTMs for the A3C algorithm. It was not really accurate though. I believe this is because I did not normalize my data. I would like to come back to this project and try again some day"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

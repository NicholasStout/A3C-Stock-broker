{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import threading\n",
    "import multiprocessing\n",
    "from random import choice\n",
    "from time import sleep\n",
    "from time import time\n",
    "import scipy.signal\n",
    "from env import stock\n",
    "import collections as col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class A3C:\n",
    "    def __init__(self, opt, scope, inputs=12, outputs=3, dropout=0.5, depth=2):        \n",
    "        with tf.variable_scope(scope):\n",
    "            layers = []\n",
    "            self.inp = tf.placeholder(dtype=tf.float32, shape=[None, inputs])\n",
    "            d_layer = tf.layers.dense(self.inp, 2**(depth+2), activation=tf.nn.relu)\n",
    "            layers.append(d_layer)\n",
    "            for i in range(1, depth):\n",
    "                d_layer = tf.layers.dense(layers[-1], 2**(depth+2-i), activation=tf.nn.relu)\n",
    "            self.policy = tf.layers.dense(layers[-1], outputs, activation=tf.nn.softmax)\n",
    "            self.value = tf.layers.dense(layers[-1], 1, activation=None)\n",
    "       \n",
    "            if scope != 'global':\n",
    "                self.advantages = tf.placeholder(dtype=tf.float32, shape=[None])\n",
    "                self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "                self.actions_onehot = tf.one_hot(self.actions,outputs,dtype=tf.float32)\n",
    "                self.target_v = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "    \n",
    "                self.responsible_outputs = tf.reduce_sum(self.policy * self.actions_onehot, [1])\n",
    "        \n",
    "                self.value_loss = 0.5 * tf.reduce_sum(tf.square(self.target_v - tf.reshape(self.value,[-1])))\n",
    "                self.entropy = - tf.reduce_sum(self.policy * tf.log(self.policy+1e-50))\n",
    "                self.policy_loss = -tf.reduce_sum(tf.log(self.responsible_outputs+1e-50)*self.advantages)\n",
    "                self.loss = 0.5 * self.value_loss + self.policy_loss - self.entropy * 0.01\n",
    "    \n",
    "                local_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\n",
    "                self.gradients = tf.gradients(self.loss,local_vars)\n",
    "                self.var_norms = tf.global_norm(local_vars)\n",
    "                grads,self.grad_norms = tf.clip_by_global_norm(self.gradients,40.0)\n",
    "                \n",
    "                global_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'global')\n",
    "                self.apply_grads = opt.apply_gradients(zip(grads, global_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Worker:\n",
    "    def __init__(self, name, opt, path, global_eps, input_size=12, output_size=3):\n",
    "        self.name = \"w_\"+str(name)\n",
    "        self.number = name\n",
    "        self.path = path\n",
    "        self.opt = opt\n",
    "        self.glob_eps = global_eps\n",
    "        self.increment = self.glob_eps.assign_add(1)\n",
    "        self.rewards = []\n",
    "        self.lengths = []\n",
    "        self.mean_vals = []\n",
    "        self.a3c = A3C(opt, self.name, inputs=input_size)\n",
    "        self.update_local_ops = self.update_target_graph('global',self.name)\n",
    "        self.stock = stock()\n",
    "        self.stock.get_first_ohlc()\n",
    "    \n",
    "    def play(self,gamma,sess,coord,saver):\n",
    "        num_eps = 0\n",
    "        episode_rewards=[]\n",
    "        while not coord.should_stop():\n",
    "            sess.run(self.update_local_ops)\n",
    "            inputs = self.stock.next_data()\n",
    "            episode_buffer = []\n",
    "            episode_values = []\n",
    "            episode_frames = []\n",
    "            episode_reward = 0\n",
    "            num_obs = 0\n",
    "            #print(inputs)\n",
    "            while len(inputs) is not 0:\n",
    "                a_dist,v = sess.run([self.a3c.policy,self.a3c.value], \n",
    "                        feed_dict={self.a3c.inp:[inputs]})\n",
    "                with np.errstate(invalid='raise'):\n",
    "                    try:\n",
    "                        a = np.random.choice(a_dist[0],p=a_dist[0])\n",
    "                    except:\n",
    "                        break\n",
    "                a = np.argmax(a_dist == a)\n",
    "                r = self.stock.make_choice(a)\n",
    "                episode_buffer.append([inputs,a,r,v[0,0]])\n",
    "                episode_values.append(v[0,0])\n",
    "                episode_reward += r\n",
    "                num_obs += 1\n",
    "                inputs = self.stock.next_data()\n",
    "            #print(len(episode_buffer))\n",
    "            self.stock.get_new_day()\n",
    "            self.stock.get_first_ohlc()\n",
    "            episode_rewards.append(episode_reward)\n",
    "            if len(episode_buffer) != 0:\n",
    "                episode_buffer = np.array(episode_buffer)\n",
    "                t_inputs = episode_buffer[:,0]\n",
    "                actions = episode_buffer[:,1]\n",
    "                rewards = episode_buffer[:,2]\n",
    "                values = episode_buffer[:,3]\n",
    "                \n",
    "                self.rewards.append(episode_reward)\n",
    "                self.lengths.append(num_obs)\n",
    "                self.mean_vals.append(np.mean(episode_values))\n",
    "            \n",
    "                rewards_plus = np.asarray(rewards.tolist()+[0])\n",
    "                discounted_rewards = self.discount(rewards_plus,gamma)[:-1]\n",
    "                value_plus = np.asarray(values.tolist()+[0])\n",
    "                advantages = rewards + gamma * value_plus[1:] - value_plus[:-1]\n",
    "                advantages = self.discount(advantages,gamma)\n",
    "                \n",
    "                feed_dict = {self.a3c.inp:np.vstack(t_inputs),\n",
    "                             self.a3c.target_v:discounted_rewards,\n",
    "                             self.a3c.actions:actions,\n",
    "                             self.a3c.advantages:advantages}\n",
    "                \n",
    "                v_l,p_l,e_l,g_n,v_n,_ = sess.run([self.a3c.value_loss,\n",
    "                                                  self.a3c.policy_loss,\n",
    "                                                  self.a3c.entropy,\n",
    "                                                  self.a3c.grad_norms,\n",
    "                                                  self.a3c.var_norms,\n",
    "                                                  self.a3c.apply_grads],\n",
    "                                                  feed_dict=feed_dict)\n",
    "                \n",
    "                num_eps+=1\n",
    "                if self.name == 'w_0':\n",
    "                    sess.run(self.increment)\n",
    "                    print(col.Counter(actions))\n",
    "                    if num_eps % 10 == 0:\n",
    "                        saver.save(sess,self.path+'/model/ac.cptk')\n",
    "                        print(np.mean(episode_rewards[-5:]))\n",
    "                \n",
    "    def discount(self, x, gamma):\n",
    "        return scipy.signal.lfilter([1], [1, -gamma], x[::-1], axis=0)[::-1]\n",
    "    \n",
    "    def update_target_graph(self, from_scope,to_scope):\n",
    "        from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_scope)\n",
    "        to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_scope)\n",
    "\n",
    "        op_holder = []\n",
    "        for from_var,to_var in zip(from_vars,to_vars):\n",
    "            op_holder.append(to_var.assign(from_var))\n",
    "        return op_holder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = .99 \n",
    "s_size = 14 \n",
    "a_size = 3 \n",
    "load_model = False\n",
    "model_path = 'graph'\n",
    "\n",
    "with tf.device(\"/cpu:0\"): \n",
    "    global_episodes = tf.Variable(0,dtype=tf.int32,name='global_episodes',trainable=False)\n",
    "    trainer = tf.train.AdamOptimizer(learning_rate=1e-4)\n",
    "    master_network = A3C(trainer, 'global', inputs=s_size) # Generate global network\n",
    "    num_workers = multiprocessing.cpu_count() # Set workers to number of available CPU threads\n",
    "    workers = []\n",
    "    # Create worker classes\n",
    "    for i in range(num_workers):\n",
    "        workers.append(Worker(i, trainer, model_path, global_episodes, input_size=s_size))\n",
    "    saver = tf.train.Saver(max_to_keep=5)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    coord = tf.train.Coordinator()\n",
    "    if load_model == True:\n",
    "        print ('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    worker_threads = []\n",
    "    for worker in workers:\n",
    "        worker_work = lambda: worker.play(gamma,sess,coord,saver)\n",
    "        t = threading.Thread(target=(worker_work))\n",
    "        t.start()\n",
    "        sleep(0.5)\n",
    "        worker_threads.append(t)\n",
    "    coord.join(worker_threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This was an A3C algorithm designed to try to buy and sell stocks.  Using an external state class, it would attempt to decide whether to buy, sell, or do nothing. It was trained using the USD-GBP forex trading pair. It ultimately failed to produce an effective model. I believe the reson for this is two fold:\n",
    "\n",
    "1) The predictions from my rnn linear regression wasn't accurate enough (more on that in the rnn repo)\n",
    "2) There needed to be more data to draw a conclusion from\n",
    "\n",
    "I believe that, had I done more research in to current stock forecasting, this could have been more effective. \n",
    "\n",
    "That being said, the best I could get the model to do was to not ever buy or sell anything. This is the stategy I would suggest for anyone trying to earn money through the stock market as naively as I was."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
